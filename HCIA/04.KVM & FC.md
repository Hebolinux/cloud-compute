### Xen架构与KVM架构

![KVM&XEN架构图](https://github.com/Hebolinux/image/blob/main/cloud-compute/HCIA/KVM%20%26%20XEN/KVM%26XEN.png)

- **Xen**

  在上一节的计算虚拟化部分的I/O虚拟化下的半虚拟化架构，就是取自Xen架构，`Dom0`就是系统VM，`DomU`代表用户VM

- **KVM（Kernel-based Virtual Machine）**

  *KVM同时具备Ⅰ型和Ⅱ型虚拟化的特点*，KVM是基于linux内核实现的，是Linux内核的一个模块，可以看作一个应用。但KVM安装好后处于内核态，可直接调用硬件，这也是KVM与Ⅱ型虚拟化最大的不同。KVM的内核模块叫`kvm.ko`，**只用于管理vCPU和内存**，内核模块安装位置在`/dev/kvm`

  相较Xen，KVM是非常精简的，所以KVM只负责CPU和内存的虚拟化，但VM所需要的资源远不止CPU和内存，所以还需要`QEMU`实现对I/O设备的虚拟化。

```diff
+ 回顾Ⅱ型虚拟化架构，在HostOS上安装虚拟化软件（VMM），整个VMM都处于用户态，而Ⅰ型虚拟化架构中VMM处于内核态，且可以直接调用硬件
- /dev/kvm实际上是KVM提供的虚拟字符设备，当KVM收到VM发送的I/O指令时，通过此设备将I/O指令发送给QEMU
- 华为的官方文档中将KVM列入Ⅱ型虚拟化
```

#### QEMU

QEMU是另一个独立的软件，**运行在用户空间**，用于提供任何硬件的I/O虚拟化，由于是全软，CPU参与模拟过多导致模拟效率低。QEMU通过一个软件包与KVM整合，通过ioctl调用*kvm接口*，将CPU指令部分交由KVM，这是`qemu-kvm`。

![image-20210128112940081](C:\Users\hebor\AppData\Roaming\Typora\typora-user-images\qemu-kvm.png)

在KVM虚拟化场景中，VM的指令集到达KVM后，KVM将直接调用硬件处理CPU和内存的指令集，但由于无法处理I/O指令，KVM将其拦截，并通过`/dev/kvm`设备将I/O指令发送给QEMU处理，QEMU再通过硬件驱动调用I/O设备。

```diff
- I/O指令从VM到KVM，再从KVM到QEMU，QEMU调用驱动。这其中多次经过内核态和用户态之间的切换
```



#### Libvirt

Libvirt是目前使用最广泛的多平台虚拟机管理工具，**也处于用户态**，可以将其看作是管理工具与虚拟化平台之间的一个中间件，它是提供与多种虚拟化平台对接的API的库，开发可以通过此库函数自研针对不同虚拟化平台的管理软件，管理工具也是通过libvirt实现对VM的管控。KVM场景下，`libvirt`调用`qemu-kvm`管理VM，`qemu`和`libvirtd`均通过伪字符设备`/dev/kvm`出发内核模块

![](C:\Users\hebor\AppData\Roaming\Typora\typora-user-images\libvirt.png)



### KVM I/O操作流程

#### 一、默认流程（I/O全虚）

![image-20210128114654060](C:\Users\hebor\AppData\Roaming\Typora\typora-user-images\KVM IO操作流程-默认.png)

1. 首先GuestOS通过设备驱动向内核发起I/O请求，将请求送到`/dev/kvm`
2. KVM收到I/O请求后通过I/O共享页将请求返还到用户态的QEMU
3. QEMU调用内核中的设备原生驱动以调用硬件

```diff
+ I/O共享页实际上就是共享内存，内存是分页的，只是QEMU与KVM共享此分页
- 此架构中GuestOS的设备驱动没有分为前驱和后驱，所以不是半虚，I/O数据流也没有直通硬件，所以也不是硬虚
```



#### 二、Virtio（半虚）

![image-20210128145126447](C:\Users\hebor\AppData\Roaming\Typora\typora-user-images\KVM IO优化-半虚.png)

Virtio情景下，I/O数据不需要频繁的在内核态和用户态之间切换，改进了QEMU与VM之间的I/O吞吐

```diff
+ vring是一个环形消息队列，其本质也是内存空间，前驱直接将I/O数据放在队列中，后驱只需要读取指令，反之亦然
+ Notification表示通知机制，环形队列无论是收到前驱或后驱的I/O数据后，都要通知另一方读取数据
- 利用前后驱模型将VM与QEMU直接连接，减少KVM内核在中间的参与
```



### FusionCompute

**VRM（Virtual Resource Manager）**

​	CNA的管理端，可单独安装或作为VM安装，为管理员提供web页面（portal）对CNA进行管控。VRM为CNA提供唯一的管理入口，没有VRM无法对CNA做操作，但VRM并不影响CNA上的业务。为了避免单点故障，VRM的部署一般采用主备模式，主备之间通过心跳检测运行状态，共享GaussDB和同一个浮动IP，所以主备至少需要3个IP

​	每一台CNA节点上都有一个VNA代理，VNA专用于VRM对CNA的管理，CNA也通过此代理向VRM上报自身设备运行状态

**CNA（Compute Node Agent）**

​	虚拟化的服务器，KVM和用户的VM业务都在CNA上

[华为官方手册_6.3.1](https://support.huawei.com/hedex/hdx.do?docid=EDOC1100042941&lang=zh&idPath=22658044|7919788|9856606|21462752|8576912)
